# main.py
import re
import os
import asyncio
import shutil
import uuid
from pathlib import Path
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
from typing import List, Optional

# --- ê°€ë¹„ì§€ ì»¬ë ‰í„° ë° PyTorch ì„í¬íŠ¸ ---
import gc
import torch
# ---

import aiofiles # ë¹„ë™ê¸° íŒŒì¼ ì²˜ë¦¬
from dotenv import load_dotenv # .env íŒŒì¼ ë¡œë“œ
from fastapi import FastAPI, Request, Form, File, UploadFile, HTTPException, status
from fastapi.middleware.cors import CORSMiddleware # @CrossOrigin ëŒ€ì²´
from pydantic import BaseModel # Javaì˜ Record/DTO ëŒ€ì²´

# --- ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
from faster_whisper import WhisperModel
from llama_cpp import Llama
# ---

# --- 2. ì„¤ì • ë° ì•± ì´ˆê¸°í™” ---

load_dotenv()  # .env íŒŒì¼ ë¡œë“œ
app = FastAPI()

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# .env íŒŒì¼ì—ì„œ Llama ëª¨ë¸ ê²½ë¡œ ì½ê¸°
LLAMA_MODEL_PATH = os.getenv("LLAMA_MODEL_PATH")
if not LLAMA_MODEL_PATH:
    print("ì¹˜ëª…ì  ì˜¤ë¥˜: .env íŒŒì¼ì— LLAMA_MODEL_PATHê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    exit(1)

# ì„ì‹œ í´ë” ì„¤ì •
TEMP_DIR = Path(os.getenv("java.io.tmpdir", os.getcwd())) / "fastapi-temp"
PARTIAL_SUMMARY_DIR = TEMP_DIR / "partial-summaries"

# --- 3. [í•µì‹¬] ëª¨ë¸ì„ ì „ì—­ ë³€ìˆ˜ë¡œ ì„ ì–¸í•˜ê³  startup ì‹œ ë¡œë“œ ---

whisper_model: Optional[WhisperModel] = None
llama_model: Optional[Llama] = None

@app.on_event("startup")
async def on_startup():
    global whisper_model, llama_model
    
    # ì„ì‹œ í´ë” ìƒì„±
    os.makedirs(TEMP_DIR, exist_ok=True)
    os.makedirs(PARTIAL_SUMMARY_DIR, exist_ok=True)
    print(f"ì„ì‹œ íŒŒì¼ í´ë”: {TEMP_DIR.resolve()}")
    print(f"ì¤‘ê°„ ìš”ì•½ ì €ì¥ í´ë”: {PARTIAL_SUMMARY_DIR.resolve()}")

    # 1. Whisper ëª¨ë¸ ë¡œë“œ (ì„œë²„ ì‹œì‘ ì‹œ 1íšŒ)
    try:
        print("[Whisper] ëª¨ë¸ ë¡œë“œ ì‹œì‘ (medium, cuda)...")
        # "medium" ëª¨ë¸ì„ HuggingFaceì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ
        whisper_model = WhisperModel("medium", device="cuda", compute_type="float16")
        print("[Whisper] ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
    except Exception as e:
        print(f"ì¹˜ëª…ì  ì˜¤ë¥˜: Whisper ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        # (ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ë˜ëŠ” VRAM ë¶€ì¡± ì‹œ ì—¬ê¸°ì„œ ì„œë²„ê°€ ì¤‘ì§€ë  ìˆ˜ ìˆìŒ)

    # 2. Llama ëª¨ë¸ ë¡œë“œ (ì„œë²„ ì‹œì‘ ì‹œ 1íšŒ)
    try:
        print(f"[Llama] ëª¨ë¸ ë¡œë“œ ì‹œì‘: {LLAMA_MODEL_PATH}")
        llama_model = Llama(
            model_path=LLAMA_MODEL_PATH,
            n_gpu_layers=30,  # -1 = ê°€ëŠ¥í•œ ë§Œí¼ GPUì— ì˜¬ë¦¼
            n_ctx=4096,
            n_threads=8,
            n_batch=512,
            verbose=True # ì‹œì‘ ì‹œ ë¡œê·¸ í™•ì¸
        )
        print("[Llama] ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
    except Exception as e:
        print(f"ì¹˜ëª…ì  ì˜¤ë¥˜: Llama ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

# VRAM í•´ì œ ë¡œì§ (ì„œë²„ ì¢…ë£Œ ì‹œ)
@app.on_event("shutdown")
async def on_shutdown():
    global whisper_model, llama_model
    try:
        print("ì„œë²„ ì¢…ë£Œ... AI ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘...")
        if whisper_model: del whisper_model
        if llama_model: del llama_model
        gc.collect()
        torch.cuda.empty_cache()
        print("AI ëª¨ë¸ VRAM ì–¸ë¡œë“œ ì™„ë£Œ.")
    except Exception as e:
        print(f"ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")

# --- 4. DTO ì •ì˜ ---
class SttResponse(BaseModel):
    text: Optional[str] = None
    error: Optional[str] = None
    transcriptId: Optional[str] = None

class RetryRequest(BaseModel):
    startTime: str
    transcriptId: str

class SimpleSummaryResponse(BaseModel):
    text: Optional[str] = None
    error: Optional[str] = None

# --- 5. STT ë° Llama ì‹¤í–‰ í—¬í¼ í•¨ìˆ˜ ---
# (ëª¨ë¸ì„ ì¸ìë¡œ ë°›ì§€ ì•Šê³ , ì „ì—­ ë³€ìˆ˜ whisper_model, llama_modelì„ ì‚¬ìš©)

async def run_stt_on_file(audio_file: UploadFile) -> Optional[str]:
    """
    ì—…ë¡œë“œëœ ì˜¤ë””ì˜¤ íŒŒì¼ 1ê°œë¥¼ STT ì²˜ë¦¬ (ì „ì—­ ëª¨ë¸ ì‚¬ìš©)
    """
    global whisper_model
    if not whisper_model:
        raise HTTPException(status_code=503, detail="Whisper ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    temp_file_path = None
    try:
        temp_file_path = TEMP_DIR / f"stt-in-{uuid.uuid4()}.webm"
        async with aiofiles.open(temp_file_path, 'wb') as f:
            content = await audio_file.read()
            await f.write(content)

        def transcribe_sync():
            segments, info = whisper_model.transcribe(str(temp_file_path), language="ko", beam_size=5)
            return " ".join([segment.text.strip() for segment in segments if segment.text.strip()])

        print(f"Whisper STT ì‹œì‘: {audio_file.filename}")
        transcript_result = await asyncio.to_thread(transcribe_sync)
        print(f"Whisper STT ì™„ë£Œ: {audio_file.filename}")
        
        if not transcript_result.strip():
            print(f"Whisper ê²°ê³¼ ì—†ìŒ ({audio_file.filename})")
            return None

        filename_no_ext = (audio_file.filename or "").rsplit('.', 1)[0]
        return f"{filename_no_ext}: {transcript_result}"
    except Exception as e:
        print(f"ì˜¤ë¥˜: {audio_file.filename} STT ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸: {e}")
        return None
    finally:
        if temp_file_path and os.path.exists(temp_file_path):
            try: os.remove(temp_file_path)
            except Exception as e: print(f"ì˜¤ë¥˜: ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")

async def run_stt(audio_files: List[UploadFile]) -> str:
    """
    ì˜¤ë””ì˜¤ íŒŒì¼ ëª©ë¡ì„ ë³‘ë ¬ë¡œ STT ì²˜ë¦¬
    """
    global whisper_model
    if not whisper_model:
        raise HTTPException(status_code=503, detail="Whisper ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    tasks = [run_stt_on_file(file) for file in audio_files] # whisper_model ì¸ì ì œê±°
    results = await asyncio.gather(*tasks)
    return "\n".join(r for r in results if r)

async def get_summary_from_llama(transcript: str, meeting_start_time_str: str, chunk_start_time_str: Optional[str] = None) -> str:
    """
    Llama ëª¨ë¸ì„ ì‹¤í–‰í•˜ì—¬ 'ì¤‘ê°„ ìš”ì•½' (í…Œì´ë¸”) ìƒì„± (ì „ì—­ ëª¨ë¸ ì‚¬ìš©)
    """
    global llama_model
    if not llama_model:
        raise HTTPException(status_code=503, detail="Llama ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # 1. íšŒì˜ ì‹œì‘ ì‹œê°„ (KST)
    try:
        utc_meeting_start = datetime.fromisoformat(meeting_start_time_str)
        korean_meeting_start = utc_meeting_start.astimezone(ZoneInfo("Asia/Seoul"))
        formatted_meeting_start_time = korean_meeting_start.strftime("%H:%M:%S")
    except Exception: 
        formatted_meeting_start_time = "(ì‹œê°„ ì •ë³´ ì˜¤ë¥˜)"
        korean_meeting_start = datetime.now(ZoneInfo("Asia/Seoul")) # Fallback

    # 2. [ì¶”ê°€] ì²­í¬ ì‹œì‘ ì‹œê°„ (KST) ë° ì˜¤í”„ì…‹ ê³„ì‚°
    if not chunk_start_time_str:
        chunk_start_time_str = meeting_start_time_str # Fallback (ì²« ì²­í¬)
            
    try:
        utc_chunk_start = datetime.fromisoformat(chunk_start_time_str)
        korean_chunk_start = utc_chunk_start.astimezone(ZoneInfo("Asia/Seoul"))
        # [ì¶”ê°€] íšŒì˜ ì‹œì‘ìœ¼ë¡œë¶€í„° ì´ ì²­í¬ê¹Œì§€ ëª‡ ì´ˆê°€ ì§€ë‚¬ëŠ”ì§€ ê³„ì‚°
        time_offset_seconds = (korean_chunk_start - korean_meeting_start).total_seconds()
        if time_offset_seconds < 0:
            time_offset_seconds = 0
    except Exception:
        time_offset_seconds = 0.0

    system_prompt = (
        f"ë‹¹ì‹ ì€ íšŒì˜ë¡ì„ ë¶„ì„í•˜ì—¬ ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ ìš”ì•½í•˜ëŠ” ì „ë¬¸ ë¹„ì„œì…ë‹ˆë‹¤.\n"
        f"ì´ íšŒì˜ëŠ” {formatted_meeting_start_time}ì— ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
        f"í˜„ì¬ ì œê³µë˜ëŠ” í…ìŠ¤íŠ¸(íšŒì˜ë¡ ì²­í¬)ëŠ” íšŒì˜ ì‹œì‘ í›„ ì•½ {int(time_offset_seconds)}ì´ˆê°€ ì§€ë‚œ ì‹œì ë¶€í„°ì˜ ë‚´ìš©ì…ë‹ˆë‹¤.\n"
        "ì•„ë˜ í…ìŠ¤íŠ¸ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ëŠ” ì´ ì²­í¬ ì‹œì‘(00:00) ê¸°ì¤€ì…ë‹ˆë‹¤. ì´ë¥¼ ì‹¤ì œ ì‹œê°„ìœ¼ë¡œ ê³„ì‚°í•´ì•¼ í•©ë‹ˆë‹¤.\n\n"
        "### ì£¼ìš” ê·œì¹™:\n"
        "1. **ì‹œê°„ ê³„ì‚°:** 'íšŒì˜ ì‹œì‘ ì‹œê°„'({formatted_meeting_start_time}) + 'ì²­í¬ ì˜¤í”„ì…‹'({int(time_offset_seconds)}ì´ˆ) + 'ì²­í¬ ë‚´ íƒ€ì„ìŠ¤íƒ¬í”„'ë¥¼ ë”í•˜ì—¬ ì‹¤ì œ ì‹œê°„ì„ ê³„ì‚°í•˜ì„¸ìš”.\n"
        "2. **ë°œì–¸ ë³‘í•©:** ê°™ì€ ë°œì–¸ìê°€ ì—°ì†í•´ì„œ ë§í•˜ëŠ” ê²½ìš°, ë‚´ìš©ì„ ìš”ì•½í•˜ì—¬ **í•˜ë‚˜ì˜ í–‰ìœ¼ë¡œ í•©ì³ì•¼ í•©ë‹ˆë‹¤.** ì‹œê°„ êµ¬ê°„ì€ í•©ì³ì§„ ë°œì–¸ì˜ ì‹œì‘ ì‹œê°„ê³¼ ë ì‹œê°„ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.\n"
        "3. **ì •í™•í•œ í˜•ì‹:** ë°˜ë“œì‹œ ì•„ë˜ ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” í˜•ì‹ì— ë§ì¶° ì‘ë‹µí•˜ê³ , ë‹¤ë¥¸ ì„¤ëª…ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”. ê·¸ë¦¬ê³  ì‹œê°„ êµ¬ê°„ì— ì†Œìˆ«ì ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.\n\n"
        "### ì˜ˆì‹œ:\n"
        "| ì‹œê°„ êµ¬ê°„           | ë°œì–¸ì | í•µì‹¬ ë‚´ìš©                  |\n"
        "|---------------------|--------|----------------------------|\n"
        "| 14:30:16-14:30:45 | ì´ì˜í¬ | ë‹¤ìŒ ì£¼ê¹Œì§€ ê¸°íšì„œ ë§ˆë¬´ë¦¬ë¥¼ ì œì•ˆí•˜ê³ , UI ë””ìì¸ íŒŒíŠ¸ë¥¼ ë‹´ë‹¹í•˜ê² ë‹¤ê³  ë§í•¨. |\n\n"
    )
    user_prompt = f"ì•„ë˜ íšŒì˜ë¡ì„ ë¶„ì„í•˜ì—¬ ìš”ì•½ í…Œì´ë¸”ì„ ìƒì„±í•´ ì£¼ì„¸ìš”:\n\n---\n{transcript}"
    
    final_prompt = (
        "/no_think <|im_start|>system\n" +
        system_prompt +
        "<|im_end|>\n<|im_start|>user\n" +
        user_prompt +
        "<|im_end|>\n<|im_start|>assistant\n"
    )
    print(f"Llama ëª¨ë¸ë¡œ 'ì¤‘ê°„ ìš”ì•½' ìƒì„± ì‹œì‘ (ì˜¤í”„ì…‹: {int(time_offset_seconds)}ì´ˆ)")

    def create_completion_sync():
        return llama_model.create_completion(
            prompt=final_prompt, temperature=0.5, max_tokens=2048, stream=False
        )
    output = await asyncio.to_thread(create_completion_sync)
    summary = output['choices'][0]['text']
    summary = re.sub(r"<think>.*?</think>", "", summary, flags=re.DOTALL).strip()
    print("Llama 'ì¤‘ê°„ ìš”ì•½' ìƒì„± ì™„ë£Œ.")
    return summary or ""

async def run_simple_summary(text_to_summarize: str) -> str:
    """
    Llama ëª¨ë¸ì„ ì‹¤í–‰í•˜ì—¬ 'ë‹¨ìˆœ ìš”ì•½' ìƒì„± (ì „ì—­ ëª¨ë¸ ì‚¬ìš©)
    """
    global llama_model
    if not llama_model:
        raise HTTPException(status_code=503, detail="Llama ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # ë‹¨ìˆœ ìš”ì•½ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸
    system_prompt = (
        "ë‹¹ì‹ ì€ ìœ ìš©í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
        "ì‚¬ìš©ìê°€ ì œê³µí•œ í…ìŠ¤íŠ¸ë¥¼ í•µì‹¬ ë‚´ìš©ë§Œ ë½‘ì•„ì„œ **ë§ˆí¬ë‹¤ìš´ í˜•ì‹**, **Notion ìŠ¤íƒ€ì¼**ë¡œ ì£¼ì œë³„ë¡œ ë¬¸ë‹¨ì„ ë‚˜ëˆ„ê³  ì¤‘ìš” ë‚´ìš©ì€ ê¸€ë¨¸ë¦¬ ê¸°í˜¸(-)ì™€ ğŸ’¡, ğŸ“…, ğŸ‘¤ ê°™ì€ ì´ëª¨ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¹”ë”í•˜ê²Œ ì •ë¦¬í•˜ì„¸ìš”.\n"
        "**ì¤‘ìš”:** ì‘ë‹µì„ ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡(```)ìœ¼ë¡œ ì ˆëŒ€ ê°ì‹¸ì§€ ë§ˆì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ ìš”ì•½ëœ ë§ˆí¬ë‹¤ìš´ ë‚´ìš© ì›ë³¸ìœ¼ë¡œ ë°”ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤.\n"
    )
    user_prompt = f"ì•„ë˜ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•´ ì£¼ì„¸ìš”:\n\n---\n{text_to_summarize}"
    
    final_prompt = (
        "/no_think <|im_start|>system\n" +
        system_prompt +
        "<|im_end|>\n<|im_start|>user\n" +
        user_prompt +
        "<|im_end|>\n<|im_start|>assistant\n"
    )
    print("Llama ëª¨ë¸ë¡œ 'ë‹¨ìˆœ ìš”ì•½' ìƒì„± ì‹œì‘...")

    def create_completion_sync():
        return llama_model.create_completion(
            prompt=final_prompt, 
            temperature=0.5,
            max_tokens=1024, # ìš”ì•½ì— í•„ìš”í•œ í† í°
            stream=False
        )
    output = await asyncio.to_thread(create_completion_sync)
    summary = output['choices'][0]['text']
    summary = re.sub(r"<think>.*?</think>", "", summary, flags=re.DOTALL).strip()
    print("Llama 'ë‹¨ìˆœ ìš”ì•½' ìƒì„± ì™„ë£Œ.")
    return summary or "ìš”ì•½ ë‚´ìš©ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

async def get_final_report_from_llama(all_partial_summaries: str, start_time_str: str, end_time_str: Optional[str] = None) -> str:
    """
    Llama ëª¨ë¸ì„ ì‹¤í–‰í•˜ì—¬ 'ìµœì¢… ë³´ê³ ì„œ' (Notion ìŠ¤íƒ€ì¼) ìƒì„± (ì „ì—­ ëª¨ë¸ ì‚¬ìš©)
    [ìˆ˜ì •] end_time_str ì¸ì ì¶”ê°€
    """
    global llama_model
    if not llama_model:
        raise HTTPException(status_code=503, detail="Llama ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # 1. íšŒì˜ ì‹œì‘ ì‹œê°„
    try:
        utc_start_time = datetime.fromisoformat(start_time_str)
        korean_start_time = utc_start_time.astimezone(ZoneInfo("Asia/Seoul"))
        formatted_start_time = korean_start_time.strftime("%Y-%m-%d %H:%M:%S")
        formatted_start_hhmmss = korean_start_time.strftime("%H:%M:%S")
    except Exception: 
        formatted_start_time = "(ì‹œì‘ ì‹œê°„ ì˜¤ë¥˜)"
        formatted_start_hhmmss = "HH:mm:ss"

    # 2. [ì¶”ê°€] íšŒì˜ ì¢…ë£Œ ì‹œê°„
    if not end_time_str:
        end_time_str = datetime.now().isoformat() # fallback
            
    try:
        utc_end_time = datetime.fromisoformat(end_time_str)
        korean_end_time = utc_end_time.astimezone(ZoneInfo("Asia/Seoul"))
        formatted_end_hhmmss = korean_end_time.strftime("%H:%M:%S")
    except Exception:
        formatted_end_hhmmss = "HH:mm:ss"


    system_prompt = (
        f"ë‹¹ì‹ ì€ íšŒì˜ì˜ ì¤‘ê°„ ìš”ì•½ë³¸ë“¤ì„ ì·¨í•©í•˜ì—¬ í•˜ë‚˜ì˜ ìµœì¢… ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ëŠ” ì „ë¬¸ ë¹„ì„œì…ë‹ˆë‹¤.\n"
        f"ì´ íšŒì˜ëŠ” {formatted_start_time} (KST)ì— ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
        "### ì§€ì‹œì‚¬í•­:\n"
        "1.  **ë³´ê³ ì„œ ì¬êµ¬ì„±:** ì œê³µëœ ëª¨ë“  ì¤‘ê°„ ìš”ì•½(ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” í˜•ì‹) ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, í•˜ë‚˜ì˜ ì¼ê´€ëœ ìµœì¢… ë³´ê³ ì„œë¥¼ **ì„œìˆ í˜•**ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n"
        "2.  **ì‹œê°„ ì •ë³´ í†µí•©:** ì¤‘ê°„ ìš”ì•½ì— ìˆëŠ” ì‹œê°„ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ íšŒì˜ ë‚´ìš©ì„ ì‹œê°„ ìˆœì„œëŒ€ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ê¸°ìˆ í•˜ì„¸ìš”. ë‹¨, ìµœì¢… ë³´ê³ ì„œ ë³¸ë¬¸ì—ëŠ” **ê°œë³„ ë°œì–¸ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.**\n"
        "3.  **Notion ìŠ¤íƒ€ì¼:** ì£¼ì œë³„ë¡œ ë¬¸ë‹¨ì„ ë‚˜ëˆ„ê³  ì¤‘ìš” ë‚´ìš©ì€ ê¸€ë¨¸ë¦¬ ê¸°í˜¸(-)ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¹”ë”í•˜ê²Œ ì •ë¦¬í•˜ì„¸ìš”.\n"
        "4.  **í•„ìˆ˜ ì„¹ì…˜:** 'ì£¼ìš” ê²°ì • ì‚¬í•­'ê³¼ 'ì‹¤í–‰ í•­ëª©(Action Items)' ì„¹ì…˜ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ê³ , ê´€ë ¨ ë‚´ìš©ì„ ê° ì„¹ì…˜ ì•„ë˜ì— ëª…í™•íˆ ìš”ì•½í•˜ì„¸ìš”.\n"
        "5.  **ì‹œê°„ ë²”ìœ„ ëª…ì‹œ (ê°€ì¥ ì¤‘ìš”):**\n"
        f"    * ë³´ê³ ì„œì˜ ê°€ì¥ ì²« ì¤„ì€ ë°˜ë“œì‹œ `## íšŒì˜ ì£¼ìš” ë‚´ìš© ({formatted_start_hhmmss} ~ {formatted_end_hhmmss})` í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.\n"
        f"    * ê´„í˜¸ ì•ˆì˜ ì‹œì‘ ì‹œê°„({formatted_start_hhmmss})ê³¼ ì¢…ë£Œ ì‹œê°„({formatted_end_hhmmss})ì€ **ì •í™•íˆ** ì œê³µëœ ê°’ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.\n"
        "    * ì‹œê°„ì€ **ë°˜ë“œì‹œ 'HH:mm:ss' í˜•ì‹**ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤. (ì˜ˆ: 15:30:05)\n"
        "    * **ì‹œê°„(HH)ì€ 00~23 ì‚¬ì´, ë¶„(mm)ê³¼ ì´ˆ(ss)ëŠ” 00~59 ì‚¬ì´ì˜ ê°’ì´ì–´ì•¼ í•©ë‹ˆë‹¤.** ì ˆëŒ€ ë‹¤ë¥¸ í˜•ì‹ì´ë‚˜ ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ ê°’ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.\n"
        "6.  **ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” ê¸ˆì§€:** ìµœì¢… ë³´ê³ ì„œì—ëŠ” ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” í˜•ì‹ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.\n\n"

        "### ìµœì¢… ë³´ê³ ì„œ ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ:\n"
        f"## íšŒì˜ ì£¼ìš” ë‚´ìš© ({formatted_start_hhmmss} ~ {formatted_end_hhmmss})\n"
        "- (ì£¼ì œ 1ì— ëŒ€í•œ ë…¼ì˜ ë‚´ìš©ì„ ì„œìˆ í˜•ìœ¼ë¡œ ì‘ì„±...)\n"
        "- (ì£¼ì œ 2ì— ëŒ€í•œ ë…¼ì˜ ë‚´ìš©ì„ ê¸€ë¨¸ë¦¬ ê¸°í˜¸ë¡œ ìš”ì•½...)\n\n"
        "## ì£¼ìš” ê²°ì • ì‚¬í•­\n"
        "- (ê²°ì •ëœ ì‚¬í•­ 1...)\n"
        "- (ê²°ì •ëœ ì‚¬í•­ 2...)\n\n"
        "## ì‹¤í–‰ í•­ëª© (Action Items)\n"
        "- (ë‹´ë‹¹ì: ë§ˆê°ì¼ - ì‹¤í–‰í•  ë‚´ìš©...)\n"
    )
    user_prompt = f"ì•„ë˜ëŠ” íšŒì˜ì˜ ì¤‘ê°„ ìš”ì•½ë³¸ë“¤ì…ë‹ˆë‹¤. ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ 'ìµœì¢… ë³´ê³ ì„œ'ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:\n\n---\n{all_partial_summaries}"

    final_prompt = (
        "/no_think <|im_start|>system\n" +
        system_prompt +
        "<|im_end|>\n<|im_start|>user\n" +
        user_prompt +
        "<|im_end|>\n<|im_start|>assistant\n"
    )
    print("Llama ëª¨ë¸ë¡œ 'ìµœì¢… ìš”ì•½' ìƒì„± ì‹œì‘...")

    def create_completion_sync():
        return llama_model.create_completion(
            prompt=final_prompt, temperature=0.5, max_tokens=2048
        )
    output = await asyncio.to_thread(create_completion_sync)
    summary = output['choices'][0]['text']
    summary = re.sub(r"<think>.*?</think>", "", summary, flags=re.DOTALL).strip()
    print("Llama 'ìµœì¢… ìš”ì•½' ìƒì„± ì™„ë£Œ.")
    return summary or ""

async def generate_final_summary(meeting_id: str, start_time: str, end_time: Optional[str] = None) -> str:
    """
    ì €ì¥ëœ ëª¨ë“  ì¤‘ê°„ ìš”ì•½ íŒŒì¼ì„ ì½ì–´ 'ìµœì¢… ìš”ì•½' ìƒì„± (ì „ì—­ Llama ëª¨ë¸ ì‚¬ìš©)
    [ìˆ˜ì •] end_time ì¸ì ì¶”ê°€
    """
    global llama_model
    if not llama_model:
        raise HTTPException(status_code=503, detail="Llama ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
    meeting_dir = PARTIAL_SUMMARY_DIR / meeting_id
    if not os.path.exists(meeting_dir):
        raise HTTPException(status_code=404, detail=f"ìš”ì•½ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {meeting_id}")

    summary_files = sorted(meeting_dir.glob("*.txt"))
    all_summaries = []
    for file_path in summary_files:
        try:
            async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                all_summaries.append(await f.read())
        except Exception as e:
            all_summaries.append("") # [ìˆ˜ì •] ì˜¤ë¥˜ ì‹œ ë¹ˆ ë¬¸ìì—´ ì¶”ê°€

    all_summaries_text = "\n\n---\n\n".join(all_summaries)
    if not all_summaries_text.strip():
        return "ìš”ì•½í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."

    # [ìˆ˜ì •] end_timeì„ get_final_report_from_llamaë¡œ ì „ë‹¬
    return await get_final_report_from_llama(all_summaries_text, start_time, end_time)

# --- 6. API ì—”ë“œí¬ì¸íŠ¸ ---

@app.post("/summary", response_model=SimpleSummaryResponse)
async def handle_simple_summary(text: str = Form(...)):
    """
    ì œê³µëœ í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ë‹¨ìˆœ ìš”ì•½ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    
    # ì „ì—­ ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
    if not llama_model:
        return SimpleSummaryResponse(error="AI ëª¨ë¸ì´ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    if not text or not text.strip():
        return SimpleSummaryResponse(error="ìš”ì•½í•  'text' ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
        
    try:
        # 1. ë‹¨ìˆœ ìš”ì•½ í—¬í¼ í•¨ìˆ˜ í˜¸ì¶œ
        summary_result = await run_simple_summary(text)
        
        # 2. ìš”ì•½ ê²°ê³¼ ë°˜í™˜
        return SimpleSummaryResponse(text=summary_result)

    except Exception as e:
        print(f"ì˜¤ë¥˜: /summary ì—”ë“œí¬ì¸íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return SimpleSummaryResponse(error=f"ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜: {e}")

@app.post("/process-audio-chunk", response_model=SttResponse)
async def handle_audio_chunk(
    meetingId: str = Form(...),
    startTime: str = Form(...),
    isFinal: bool = Form(...),
    audio_files: List[UploadFile] = File(...),
    chunkStartTime: Optional[str] = Form(None), 
    endTime: Optional[str] = Form(None)
):
    # ì „ì—­ ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
    if not whisper_model or not llama_model:
        raise HTTPException(status_code=503, detail="AI ëª¨ë¸ì´ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„œë²„ ì‹œì‘ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        
    try:
        # 1. STT ì‹¤í–‰ (ì „ì—­ ëª¨ë¸ ì‚¬ìš©)
        full_transcript = await run_stt(audio_files) # whisper_model ì¸ì ì œê±°

        # STT ê²°ê³¼ ì—†ìŒ ì²˜ë¦¬
        if not full_transcript.strip():
            print(f"STT ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ (MeetingID: {meetingId}, isFinal: {isFinal})")
            if isFinal:
                pass 
            else:
                return SttResponse()

        partial_summary = ""
        if full_transcript.strip():
            # 2. ì¤‘ê°„ ìš”ì•½ ìƒì„± (ì „ì—­ ëª¨ë¸ ì‚¬ìš©)
            try:
                partial_summary = await get_summary_from_llama(full_transcript, startTime, chunkStartTime)
            except Exception as e:
                print(f"ì˜¤ë¥˜: ì¤‘ê°„ ìš”ì•½ ìƒì„± ì‹¤íŒ¨ (MeetingID: {meetingId}, isFinal: {isFinal}): {e}")
                if not isFinal:
                    return SttResponse()
        
        # 3. ì¤‘ê°„ ìš”ì•½ íŒŒì¼ ì €ì¥ (ë™ì¼)
        meeting_dir = PARTIAL_SUMMARY_DIR / meetingId
        if partial_summary and partial_summary.strip():
            try:
                os.makedirs(meeting_dir, exist_ok=True)
                part_file_name = f"{int(datetime.now().timestamp())}_summary.txt"
                summary_file_path = meeting_dir / part_file_name
                async with aiofiles.open(summary_file_path, 'w', encoding='utf-8') as f:
                    await f.write(partial_summary)
                print(f"ì¤‘ê°„ ìš”ì•½ ì €ì¥: {summary_file_path}")
            except Exception as e:
                print(f"ì˜¤ë¥˜: ì¤‘ê°„ ìš”ì•½ ì €ì¥ ì‹¤íŒ¨ (MeetingID: {meetingId}): {e}")
                if not isFinal:
                    return SttResponse()
                print("ìµœì¢… ìš”ì•½ ì²˜ë¦¬ ì¤‘ ì¤‘ê°„ ìš”ì•½ ì €ì¥ ì‹¤íŒ¨ ë°œìƒ.")
        else:
            print(f"ìƒì„±ëœ ì¤‘ê°„ ìš”ì•½ ë‚´ìš©ì´ ì—†ì–´ íŒŒì¼ì„ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (MeetingID: {meetingId})")

        # 4. isFinal í”Œë˜ê·¸ì— ë”°ë¼ ë¶„ê¸°
        if isFinal:
            print(f"ìµœì¢… ìš”ì•½ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤ (MeetingID: {meetingId})")
            try:
                # 5. ìµœì¢… ìš”ì•½ ìƒì„± (ì „ì—­ ëª¨ë¸ ì‚¬ìš©)
                final_summary = await generate_final_summary(meetingId, startTime, endTime)

                # 6. ì¤‘ê°„ ìš”ì•½ íŒŒì¼ë“¤ ì‚­ì œ (ë™ì¼)
                try:
                    if os.path.exists(meeting_dir):
                        shutil.rmtree(meeting_dir)
                        print(f"ì¤‘ê°„ ìš”ì•½ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {meeting_dir}")
                except Exception as e:
                    print(f"ì˜¤ë¥˜: ì¤‘ê°„ ìš”ì•½ íŒŒì¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (MeetingID: {meetingId}): {e}")

                # 7. 'ìµœì¢… ìš”ì•½' ë°˜í™˜
                return SttResponse(text=final_summary)

            except Exception as e:
                print(f"ì˜¤ë¥˜: ìµœì¢… ìš”ì•½ ìƒì„± ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ (MeetingID: {meetingId}): {e}")
                import traceback
                traceback.print_exc() # ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì¶œë ¥
                return SttResponse(error=f"ìµœì¢… ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}", transcriptId=meetingId)
        else:
            # [ì¤‘ê°„ ìš”ì•½] ìš”ì²­ì¸ ê²½ìš°, ë¹ˆ ì„±ê³µ ì‘ë‹µ ë°˜í™˜
            return SttResponse()

    except Exception as e:
        # ì˜ˆìƒì¹˜ ëª»í•œ ìµœìƒìœ„ ì˜ˆì™¸ ì²˜ë¦¬
        print(f"ì˜¤ë¥˜: handle_audio_chunk ì²˜ë¦¬ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ (MeetingID: {meetingId}): {e}")
        import traceback
        traceback.print_exc()
        return SttResponse(error=f"ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜: {e}", transcriptId=meetingId if isFinal else None)


@app.post("/retry-final-summary", response_model=SttResponse)
async def handle_retry(retry_request: RetryRequest):
    """
    ì €ì¥ëœ ì¤‘ê°„ ìš”ì•½ íŒŒì¼ë“¤ë¡œ 'ìµœì¢… ìš”ì•½' ìƒì„±ì„ ì¬ì‹œë„
    """
    global llama_model # ì „ì—­ ëª¨ë¸ ì‚¬ìš©
    meeting_id = retry_request.transcriptId
    start_time = retry_request.startTime
    meeting_dir = PARTIAL_SUMMARY_DIR / meeting_id

    if not os.path.exists(meeting_dir):
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND, 
            detail="ì €ì¥ëœ ì¤‘ê°„ ìš”ì•½ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        )
    
    if not llama_model:
        raise HTTPException(status_code=503, detail="Llama ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    try:
        # 1. ìµœì¢… ìš”ì•½ ìƒì„± (ì „ì—­ ëª¨ë¸ ì‚¬ìš©)
        # ì¬ì‹œë„ ì‹œì ì˜ ì‹œê°„ì„ endTimeìœ¼ë¡œ ì „ë‹¬
        end_time = datetime.now().isoformat()
        final_summary = await generate_final_summary(meeting_id, start_time, end_time)

        # 2. ì¤‘ê°„ ìš”ì•½ íŒŒì¼ ì‚­ì œ
        try:
            shutil.rmtree(meeting_dir)
            print(f"ì¤‘ê°„ ìš”ì•½ íŒŒì¼ ì‚­ì œ ì™„ë£Œ (ì¬ì‹œë„): {meeting_dir}")
        except Exception as e:
            print(f"ì˜¤ë¥˜: ì¤‘ê°„ ìš”ì•½ íŒŒì¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ì¬ì‹œë„) (MeetingID: {meeting_id}): {e}")
        
        # 3. ì„±ê³µ ì‘ë‹µ
        return SttResponse(text=final_summary)

    except Exception as e:
        print(f"ì˜¤ë¥˜: ìµœì¢… ìš”ì•½ ì¬ì‹œë„ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ (MeetingID: {meeting_id}): {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ìµœì¢… ìš”ì•½ ì¬ì‹œë„ ì‹¤íŒ¨: {e}"
        )
    

# --- 7. (ì„ íƒ) uvicornìœ¼ë¡œ ë°”ë¡œ ì‹¤í–‰ ---
if __name__ == "__main__":
    import uvicorn
    # Python main.pyë¥¼ ì§ì ‘ ì‹¤í–‰í•  ê²½ìš° (ê°œë°œìš©)
    print(f"Llama ëª¨ë¸ ê²½ë¡œ í™•ì¸: {LLAMA_MODEL_PATH}")
    uvicorn.run(app, host="0.0.0.0", port=8081, log_level="info")